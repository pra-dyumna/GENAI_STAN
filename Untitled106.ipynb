{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMTAjHQ25aXGLmE6uWt/GQ+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pra-dyumna/GENAI_STAN/blob/main/Untitled106.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7IOLm8_joG5N"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets accelerate torch peft\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "# Load the dataset\n",
        "def load_custom_dataset(json_file):\n",
        "    with open(json_file, 'r') as f:\n",
        "        data = json.load(f)\n",
        "    questions = [entry[\"question\"] for entry in data]\n",
        "    answers = [entry[\"answer\"] for entry in data]\n",
        "    return Dataset.from_dict({\"question\": questions, \"answer\": answers})\n",
        "\n",
        "# Preprocess the dataset\n",
        "def preprocess_function(examples, tokenizer, max_length=128):\n",
        "    inputs = examples['question']\n",
        "    targets = examples['answer']\n",
        "    model_inputs = tokenizer(inputs, max_length=max_length, truncation=True, padding=\"max_length\")\n",
        "    labels = tokenizer(targets, max_length=max_length, truncation=True, padding=\"max_length\")\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "# Load dataset\n",
        "dataset_path = \"/content/dataset.json\"  # Path to your JSON dataset file\n",
        "dataset = load_custom_dataset(dataset_path)\n",
        "\n",
        "# Split the dataset into train and validation\n",
        "train_test_split = dataset.train_test_split(test_size=0.1, seed=42)\n",
        "train_dataset = train_test_split[\"train\"]\n",
        "val_dataset = train_test_split[\"test\"]\n",
        "\n",
        "# Load tokenizer and model\n",
        "model_name = \"t5-small\"  # You can use t5-base or other variants for larger models\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# Apply LoRA\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
        "    inference_mode=False,\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Tokenize dataset\n",
        "train_dataset = train_dataset.map(lambda x: preprocess_function(x, tokenizer), batched=True)\n",
        "val_dataset = val_dataset.map(lambda x: preprocess_function(x, tokenizer), batched=True)\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./t5_lora_finetuned\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=3e-4,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=40,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=2,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    save_steps=500,\n",
        "    save_strategy=\"epoch\",\n",
        "    report_to=\"none\",\n",
        "    load_best_model_at_end=True,\n",
        "    fp16=True,  # Use mixed precision for faster training\n",
        ")\n",
        "\n",
        "# Define the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "# Fine-tune the model\n",
        "trainer.train()\n",
        "\n",
        "# Save the LoRA fine-tuned model\n",
        "model.save_pretrained(\"./t5_lora_finetuned\")\n",
        "tokenizer.save_pretrained(\"./t5_lora_finetuned\")\n"
      ],
      "metadata": {
        "id": "Olq-U249oMCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "# Load the fine-tuned model\n",
        "model_name = \"./t5_lora_finetuned\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# Perform inference\n",
        "def generate_answer(question):\n",
        "    inputs = tokenizer(question, return_tensors=\"pt\", max_length=128, truncation=True)\n",
        "    outputs = model.generate(inputs.input_ids, max_length=256, num_beams=4, early_stopping=True)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Example question\n",
        "question = \"Provide detailed information about Infosys as an IT company.\"\n",
        "answer = generate_answer(question)\n",
        "print(\"Answer:\", answer)\n"
      ],
      "metadata": {
        "id": "noX6fGaaorQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/./t5_lora_finetuned.zip /content/./t5_lora_finetuned"
      ],
      "metadata": {
        "id": "yOse46Ugvt19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import torch\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, TaskType, get_peft_model\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from datasets import Dataset\n",
        "\n",
        "# Load Dataset from JSON File\n",
        "# Replace 'dataset.json' with the path to your single dataset file.\n",
        "dataset = load_dataset(\"json\", data_files={\"full\": \"/content/dataset.json\"})[\"full\"]\n",
        "\n",
        "# Split Dataset into Training and Validation\n",
        "train_size = 0.9  # 90% for training, 10% for validation\n",
        "\n",
        "# Convert the dataset to a list of dictionaries before splitting\n",
        "dataset_list = [row for row in dataset]\n",
        "train_dataset_list, val_dataset_list = train_test_split(dataset_list, test_size=1-train_size, shuffle=True)\n",
        "\n",
        "\n",
        "# Convert back to Dataset object\n",
        "dataset = {\n",
        "    \"train\": Dataset.from_list(train_dataset_list),\n",
        "    \"validation\": Dataset.from_list(val_dataset_list),\n",
        "}\n",
        "\n",
        "# Load T5-Base Model and Tokenizer\n",
        "model_name = \"t5-base\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# Tokenization Function\n",
        "def preprocess_function(examples):\n",
        "    inputs = [f\"Question: {q} Answer:\" for q in examples[\"question\"]]\n",
        "    targets = [a for a in examples[\"answer\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\")\n",
        "    labels = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\").input_ids\n",
        "    model_inputs[\"labels\"] = labels\n",
        "    return model_inputs\n",
        "\n",
        "# Preprocess Dataset\n",
        "train_dataset = dataset[\"train\"].map(preprocess_function, batched=True)\n",
        "val_dataset = dataset[\"validation\"].map(preprocess_function, batched=True)\n",
        "\n",
        "# LoRA Configuration\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
        "    inference_mode=False,\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1\n",
        ")\n",
        "\n",
        "# Apply LoRA to the model\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Training Arguments\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./t5_lora_finetuned\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-4,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=30,\n",
        "    predict_with_generate=True,\n",
        "    logging_dir=\"./logs\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"loss\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_steps=100,\n",
        "    max_grad_norm=1.0,\n",
        "    fp16=True,\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "# Fine-Tuning\n",
        "trainer.train()\n",
        "\n",
        "# Save Model\n",
        "trainer.save_model(\"./t5_lora_finetuned_base\")\n",
        "tokenizer.save_pretrained(\"./t5_lora_finetuned_base\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "NlF3BcnrvdG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Fine-Tuned Model\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "model_name = \"./t5_lora_finetuned_base\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# Generate Answer\n",
        "def generate_answer(question, max_length=256, num_beams=6):\n",
        "    inputs = tokenizer(f\"Question: {question} Answer:\", return_tensors=\"pt\", max_length=128, truncation=True)\n",
        "    outputs = model.generate(\n",
        "        inputs.input_ids,\n",
        "        max_length=max_length,\n",
        "        num_beams=num_beams,\n",
        "        early_stopping=True,\n",
        "    )\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Example Usage\n",
        "question = \"To calculate yearly earnings, multiply the monthly earnings by 12.\"\n",
        "answer = generate_answer(question)\n",
        "print(f\"Q: {question}\\nA: {answer}\")\n"
      ],
      "metadata": {
        "id": "45f_u97mvU-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zb5h2NVt1Mw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/./t5_lora_finetuned_base.zip /content/./t5_lora_finetuned_base"
      ],
      "metadata": {
        "id": "3A9UmreVxzsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: no no give context and prompt both\n",
        "\n",
        "# Load the fine-tuned model\n",
        "model_name = \"./t5_lora_finetuned_base\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# Generate Answer\n",
        "def generate_answer(question, max_length=256, num_beams=6):\n",
        "    inputs = tokenizer(f\"Question: {question} Answer:\", return_tensors=\"pt\", max_length=128, truncation=True)\n",
        "    outputs = model.generate(\n",
        "        inputs.input_ids,\n",
        "        max_length=max_length,\n",
        "        num_beams=num_beams,\n",
        "        early_stopping=True,\n",
        "    )\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Example Usage\n",
        "question = \"\"\n",
        "answer = generate_answer(question)\n",
        "print(f\"Q: {question}\\nA: {answer}\")"
      ],
      "metadata": {
        "id": "7oJSk18fygFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: this model not give the perfect result can be prompt tuning the model no fine tuning\n",
        "\n",
        "import torch\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "# Load the fine-tuned model\n",
        "model_name = \"./t5_lora_finetuned_base\"  # Assuming this is your best performing model\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# Generate Answer\n",
        "def generate_answer(question, max_length=256, num_beams=6):\n",
        "    inputs = tokenizer(f\"Question: {question} Answer:\", return_tensors=\"pt\", max_length=128, truncation=True)\n",
        "    outputs = model.generate(\n",
        "        inputs.input_ids,\n",
        "        max_length=max_length,\n",
        "        num_beams=num_beams,\n",
        "        early_stopping=True,\n",
        "    )\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Example Usage (you can change this to any question)\n",
        "question = \"What are the benefits of using a TPU?\"\n",
        "answer = generate_answer(question)\n",
        "print(f\"Q: {question}\\nA: {answer}\")"
      ],
      "metadata": {
        "id": "2wSjiSa23hNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vpwTQSsr3o6z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}