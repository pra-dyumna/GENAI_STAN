{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOhFUDfb6zpo1+m8e0RIbW2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pra-dyumna/GENAI_STAN/blob/main/llama-68mquntized.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3IUDsDSjbPS6"
      },
      "outputs": [],
      "source": [
        "pip install --upgrade --upgrade-strategy eager \"optimum[openvino]\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.44\n",
        "!pip install openvino==24.3\n",
        "!pip install openvino-tokenizers==24.3\n",
        "!pip install optimum-intel==1.20\n",
        "!pip install lm-eval==0.4.3\n"
      ],
      "metadata": {
        "id": "ZsuqNq6YbWO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4HO-7m94capK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from optimum.intel import OVModelForCausalLM\n",
        "\n",
        "model_id = \"JackFram/llama-68m\"\n",
        "model = OVModelForCausalLM.from_pretrained(model_id, export=True)\n",
        "model.save_pretrained(\"./llama-68m-ov\")\n"
      ],
      "metadata": {
        "id": "hcDMT5sDbhP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from optimum.intel import OVModelForCausalLM, OVWeightQuantizationConfig\n",
        "\n",
        "MODEL_ID = \"JackFram/llama-68m\"\n",
        "quantization_config = OVWeightQuantizationConfig(bits=4, awq=True, scale_estimation=True, group_size=64, dataset=\"c4\")\n",
        "model = OVModelForCausalLM.from_pretrained(MODEL_ID, export=True, quantization_config=quantization_config)\n",
        "model.save_pretrained(\"./llama-68m-ov\")"
      ],
      "metadata": {
        "id": "azEbbA7RgXRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from optimum.intel import OVModelForCausalLM\n",
        "from transformers import pipeline, AutoTokenizer\n",
        "\n",
        "# Load the saved quantized model\n",
        "model = OVModelForCausalLM.from_pretrained(\"./llama-68m-ov\")\n",
        "\n",
        "# Load the tokenizer explicitly\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"JackFram/llama-68m\")  # Use the same model ID\n",
        "\n",
        "# Define a pipeline for question answering, including the tokenizer\n",
        "qa_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Ask a question\n",
        "question = \"What is the capital of France?\"\n",
        "answer = qa_pipeline(question)\n",
        "\n",
        "# Print the answer\n",
        "print(answer)"
      ],
      "metadata": {
        "id": "NDiVI_CKg3B7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login\n",
        "from huggingface_hub import HfApi\n",
        "api = HfApi()\n",
        "api.upload_folder(\n",
        "    folder_path=\"./llama-68m-ov\", # Path to your model folder\n",
        "    repo_id=\"upadhyay/llama-68m_q4\", # Replace with your Hugging Face username and model name\n",
        "    token=\"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\"\n",
        ")"
      ],
      "metadata": {
        "id": "n7jUOL5FiE_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6Eosjwc-jmC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lDuOzU_5jcw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Yg2ndR7ch5UZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}